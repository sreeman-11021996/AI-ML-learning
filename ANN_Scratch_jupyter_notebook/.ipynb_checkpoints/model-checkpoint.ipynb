{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5bfhlHd3nz8q",
    "outputId": "75ad4daf-4162-48ba-e2d4-37475ef73b93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: import-ipynb in c:\\users\\sreeman\\anaconda3\\lib\\site-packages (0.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install import-ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4JyHwgajmG4t"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "aYU-b6w3my0k"
   },
   "outputs": [],
   "source": [
    "# os.chdir(\"/content/drive/MyDrive/AI_and_ML/ANN_scratch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "5b5ljx8XmbbN",
    "outputId": "598dda91-a942-4151-cf5f-93817e4df45d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\SREEMAN\\\\AI_and_Machine_Learning\\\\0.Learning AI & ML\\\\0.5.100_days_of_Deep_Learning\\\\ANN\\\\ANN_scratch_jupyter_notebook'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "IqaruK49nq5M"
   },
   "outputs": [],
   "source": [
    "import import_ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g7M841wSnVV1",
    "outputId": "84e65e2b-9e3a-464e-96cb-fa2dade87f8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from activation.ipynb\n",
      "importing Jupyter notebook from loss.ipynb\n",
      "importing Jupyter notebook from layers.ipynb\n"
     ]
    }
   ],
   "source": [
    "from activation import Activation\n",
    "from loss import Loss\n",
    "from layers import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "CSArIJYXqwkm"
   },
   "outputs": [],
   "source": [
    "from abc import ABC,abstractmethod\n",
    "import numpy as np\n",
    "from typing import Tuple,List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(ABC):\n",
    "    \"\"\"Abstract Base Model Class\"\"\"\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def learning_rate(self):\n",
    "        ...\n",
    "    \n",
    "    @learning_rate.setter\n",
    "    @abstractmethod\n",
    "    def learning_rate(self,value:float):\n",
    "        ...\n",
    "        \n",
    "    @abstractmethod\n",
    "    def __call__(self,input_tesor:np.ndarray)->np.ndarray:\n",
    "        ...\n",
    "    \n",
    "    @abstractmethod\n",
    "    def fit(self,X_train:np.ndarray,y_train:np.ndarray,epochs:int)\\\n",
    "    ->np.ndarray:\n",
    "        ...\n",
    "    \n",
    "    @abstractmethod\n",
    "    def predict(self,X_test:np.ndarray)->np.ndarray:\n",
    "        ...\n",
    "    \n",
    "    @abstractmethod\n",
    "    def evaluate(self, X_test:np.ndarray, y_test:np.ndarray)->np.ndarray:\n",
    "        ...\n",
    "        \n",
    "    @abstractmethod\n",
    "    def update(self):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(Model):\n",
    "    \"\"\"\n",
    "    Attributes: \n",
    "        layers : List[Tuple[Layer,Activation]]\n",
    "        loss : Loss\n",
    "        learning_rate : float\n",
    "        regularization_factor : float\n",
    "        l2_reg : bool - upon false l1 reg is added\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__ (self,\n",
    "                  layers : Tuple[Tuple[Layer,Activation]],\n",
    "                  loss : Loss,\n",
    "                  learning_rate : float,\n",
    "                  regularization_factor:float,\n",
    "                  l2_reg:bool = True,\n",
    "                  threshold = 0.5):\n",
    "        \n",
    "        self._layers = layers\n",
    "        self._num_layers = len(layers)\n",
    "        self._loss = loss\n",
    "        self.learning_rate = learning_rate\n",
    "        self._num_examples = None\n",
    "        self._input = None\n",
    "        self._output = None\n",
    "        self._regularization_factor = regularization_factor\n",
    "        self._l2_reg = l2_reg\n",
    "        self._threshold = threshold\n",
    "    \n",
    "    @property\n",
    "    def learning_rate(self):\n",
    "        return self._learning_rate\n",
    "    \n",
    "    @learning_rate.setter\n",
    "    def learning_rate(self,value:float):\n",
    "        self._learning_rate = value\n",
    "        \n",
    "    def __call__ (self,input_tensor:np.ndarray)->np.ndarray:\n",
    "        \"\"\"\n",
    "        Forward Propogation ->\n",
    "        Attribute:  input_tensor: (num_features, num_examples)\n",
    "        return: _output (num_units_of_final_layer, num_examples)\n",
    "        \"\"\"\n",
    "        if self._num_examples is None:\n",
    "            self._num_examples = input_tensor.shape[-1]\n",
    "            \n",
    "        output = input_tensor\n",
    "        \n",
    "        for layer,activation in self._layers:\n",
    "            output = layer(output)\n",
    "            output = activation(output)\n",
    "            \n",
    "        self._output = output\n",
    "        \n",
    "        return self._output\n",
    "    \n",
    "    def _regulariztion(self,gradients:np.ndarray,weights:np.ndarray)\\\n",
    "    ->np.ndarray:\n",
    "        if self._l2_reg:\n",
    "            reg = (self._regularization_factor/self._num_examples)*weights\n",
    "        else:\n",
    "            reg = (self._regularization_factor/self._num_examples)\n",
    "        \n",
    "        return gradients + reg\n",
    "    \n",
    "    def backward_prop(self,y_train:np.ndarray):\n",
    "        # d_L_y^\n",
    "        d_loss_y_pred = self._loss.gradient(self._output,y_train)\n",
    "        d_a = d_loss_y_pred\n",
    "        \n",
    "        # do back_prop from last layer \n",
    "        for index in reversed(range(0,self._num_layers)):\n",
    "            layer,activation = self._layers[index]\n",
    "            \n",
    "            # A[index-1]\n",
    "            if index == 0:\n",
    "                prev_layer_output = self._input\n",
    "            else:\n",
    "                prev_layer,prev_activation = self._layers[index-1]\n",
    "                prev_layer_output = prev_activation(prev_layer.output)\n",
    "            \n",
    "            d_z = np.multiply(d_a, activaiton.gradients(layer.output))\n",
    "            \n",
    "            # d_w\n",
    "            layer.grad_weights = (\n",
    "                np.dot(d_z,np.transpose(prev_layer_output))/self._num_examples)\n",
    "            layer.grad_weights = self._regulariztion(layer.grad_weights,layer.weights)\n",
    "            \n",
    "            # d_b - (keep result as a tensor)\n",
    "            layer.grad_bias = np.mean(d_z, axis=1, keepdims=True)\n",
    "            \n",
    "            # update d_a\n",
    "            d_a = np.dot(np.transpose(layer.grad_weights),d_z)\n",
    "            \n",
    "    def fit(self,\n",
    "            X_train:np.ndarray, \n",
    "            y_train:np.ndarray,\n",
    "            epochs:int,\n",
    "            verbose:bool = True)->np.ndarray:\n",
    "        \n",
    "        self._input = X_train\n",
    "        \n",
    "        for epoch in range(1, epochs+1):\n",
    "            # Forward Prop -> self.__call__(input_tensor=self._input)\n",
    "            _ = self(self._input)    \n",
    "            # Calculate the Loss\n",
    "            loss = self._loss(self._output,y_train)\n",
    "            # Back Prop\n",
    "            self.backward_prop(y_train)\n",
    "            # update learning rate -> not finished!\n",
    "            self.update()\n",
    "            \n",
    "            # Verbose\n",
    "            if verbose:\n",
    "                print(f\"Epoch: {epoch:03d}, Loss {loss:0.4f}\")\n",
    "    \n",
    "    def predict(self,X_test:np.ndarray)->np.ndarray:\n",
    "        outputs = self(X_test)\n",
    "        return (outputs > self._threshold).astype(\"uint8\")\n",
    "    \n",
    "    def evaluate(self, X_test:np.ndarray, y_test:np.ndarray)->np.ndarray:\n",
    "        _ = self(X_test)\n",
    "        loss = self._loss(self._output,y_train)\n",
    "        return loss\n",
    "        \n",
    "    def update(self):\n",
    "        for layer,_ in self._layers:\n",
    "            layer.update(self._learning_rate)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pending Functionality\n",
    "# 0. Accuracy\n",
    "# 1. Batch Learning\n",
    "# 2. Momentum (optimizers)\n",
    "# 3. variable Learning rates\n",
    "# 4. 2 hidden layer NN -> chain diff.\n",
    "# 5. callback.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_b explanation\n",
    "# np.array([[1,2,3]]).shape\n",
    "# np.mean(np.array([[1,2,3]]),axis=1,keepdims=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
